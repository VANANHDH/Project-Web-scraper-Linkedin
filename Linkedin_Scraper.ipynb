{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4d9113ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup\n",
    "from time import sleep\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a7ff889d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "====== WebDriver manager ======\n",
      "Current google-chrome version is 94.0.4606\n",
      "Get LATEST driver version for 94.0.4606\n",
      "Driver [C:\\Users\\USER\\.wdm\\drivers\\chromedriver\\win32\\94.0.4606.41\\chromedriver.exe] found in cache\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-Finish initializing a driver\n",
      "-Finish importing the login credential\n",
      "-Finish logging in\n"
     ]
    }
   ],
   "source": [
    "# Open Chrome and access to Linkedin\n",
    "driver = webdriver.Chrome(ChromeDriverManager().install())\n",
    "url = \"https://www.linkedin.com/\"\n",
    "driver.get(url)\n",
    "print('-Finish initializing a driver')\n",
    "\n",
    "# Prevent timeout\n",
    "driver.implicitly_wait(20) # gives an implicit wait for 20 seconds\n",
    "\n",
    "# Import username and password\n",
    "credential = open('id.txt')\n",
    "line = credential.readlines()\n",
    "username = line[0]\n",
    "password = line[1]\n",
    "print('-Finish importing the login credential')\n",
    "\n",
    "# Key in username\n",
    "email_field = driver.find_element_by_id(\"session_key\")\n",
    "email_field.send_keys(username)\n",
    "sleep(2)\n",
    "\n",
    "# Key in password\n",
    "password_field = driver.find_element_by_id(\"session_password\")\n",
    "password_field.send_keys(password)\n",
    "sleep(3)\n",
    "\n",
    "# Click the Sign in button\n",
    "login_field = driver.find_element_by_xpath(r'''//*[@id=\"main-content\"]/section[1]/div[2]/form/button''')\n",
    "login_field.click()\n",
    "print('-Finish logging in')\n",
    "sleep(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e9e72a64",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What do you want to scrape? data analyst people\n",
      "-Finish search\n",
      "How many pages you want to scrape: 1\n",
      "-Finish get URL one page\n",
      "-Finish getting all URLs\n"
     ]
    }
   ],
   "source": [
    "# Input the search query to search bar\n",
    "search_field = driver.find_element_by_xpath(r'''//*[@id=\"global-nav-typeahead\"]/input''')\n",
    "\n",
    "search_query = input(\"What do you want to scrape? \")\n",
    "search_field.send_keys(search_query)\n",
    "sleep(2)\n",
    "\n",
    "# Press Enter to search\n",
    "search_field.send_keys(Keys.RETURN)\n",
    "print(\"-Finish search\")\n",
    "\n",
    "# Read one page source and get URL of profile\n",
    "def GetURL():\n",
    "    page_source = BeautifulSoup(driver.page_source, features=\"html.parser\")\n",
    "    driver.implicitly_wait(10)\n",
    "\n",
    "    # Find URL in page source \n",
    "    all_profile_URL = []\n",
    "    profiles = page_source.find_all('a', class_='app-aware-link')\n",
    "    for profile in profiles:\n",
    "        profile_URL = profile.get('href')\n",
    "        if profile_URL not in all_profile_URL:\n",
    "            all_profile_URL.append(profile_URL)\n",
    "    return all_profile_URL\n",
    "\n",
    "# Loop and find in (number_of_page)\n",
    "\n",
    "input_page = int(input('How many pages you want to scrape: '))\n",
    "URLs_all_page = []\n",
    "for page in range(input_page):\n",
    "    URLs_one_page = GetURL()\n",
    "    sleep(2)\n",
    "    driver.execute_script('window.scrollTo(0, document.body.scrollHeight)')\n",
    "    driver.implicitly_wait(5)\n",
    "    next_button = driver.find_element_by_class_name(\"artdeco-pagination__button--next\")\n",
    "    next_button.click()\n",
    "    driver.implicitly_wait(5)\n",
    "    URLs_all_page = URLs_all_page + URLs_one_page\n",
    "    print('-Finish get URL one page')\n",
    "    sleep(2)\n",
    "\n",
    "print('-Finish getting all URLs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25cde66b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diep Nguyen (Chloe)\n",
      "RMIT University Vietnam\n",
      "People Data Analyst\n",
      "Vietnam\n",
      "\n",
      "\n",
      "Brian O'Reilly\n",
      "Vietnamese-German University\n",
      "My passion is to help business people learn and develop their skills through my work in higher education and also honorary roles in the Vietnam Business Forum, AusCham, AmCham and EuroCham.\n",
      "Ho Chi Minh City, Vietnam\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Scrape data of Linkedin profile\n",
    "# Write to csv file\n",
    "with open('output.csv', mode='w', encoding='utf-8', newline = '') as file_output:\n",
    "    headers = ['name', 'school', 'title', 'location', 'URL']\n",
    "    writer = csv.DictWriter(file_output, delimiter=',', lineterminator='\\n', fieldnames=headers)\n",
    "    writer.writeheader()\n",
    "\n",
    "    for linkedin_URL in URLs_all_page:\n",
    "        try:\n",
    "            driver.get(linkedin_URL)\n",
    "            sleep(2)\n",
    "            page_source = BeautifulSoup(driver.page_source, features=\"html.parser\")\n",
    "            info_div = page_source.find('div', class_ = \"display-flex justify-space-between pt2\")\n",
    "\n",
    "            name = info_div.find('h1').get_text().strip()\n",
    "            print(name)\n",
    "\n",
    "            school = info_div.find('h2').get_text().strip()\n",
    "            print(school)\n",
    "\n",
    "            loc_title = info_div.find_all(class_='text-body-medium break-words')\n",
    "            for a in loc_title:\n",
    "                title = a.get_text().strip()\n",
    "            print(title)\n",
    "\n",
    "            loc_location = info_div.find_all(class_='text-body-small inline t-black--light break-words')\n",
    "            for b in loc_location:\n",
    "                location = b.get_text().strip()\n",
    "            print(location)\n",
    "            print('\\n')\n",
    "            \n",
    "            with open('output.csv', mode='a', encoding='utf-8', newline = '') as file_output:\n",
    "                writer = csv.DictWriter(file_output, delimiter=',', lineterminator='\\n', fieldnames=headers)\n",
    "                writer.writerow({headers[0]:name, headers[1]:school, headers[2]:title, headers[3]:location, headers[4]:linkedin_URL})\n",
    "\n",
    "        except (AttributeError):\n",
    "            print('This URL is not a profile\\n')\n",
    "        except (WebDriverException):\n",
    "            print('Webdriver is closed manually')\n",
    "\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95ae720e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c10bbb5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
